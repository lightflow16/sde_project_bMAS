# Metrics Comparison Report Generator

## Overview

The `generate_metrics_comparison_report.py` script generates a comprehensive side-by-side comparison report showing **all 60 metrics** across all 4 MAS systems from recent experiment runs.

## Usage

### Basic Usage

Simply run the script:

```bash
python generate_metrics_comparison_report.py
```

This will:
1. Scan the `metrics_outputs/` directory for recent metrics files
2. Load the most recent metrics file from each system
3. Generate a comprehensive comparison report: `metrics_comparison_report.txt`

### What the Report Includes

The report shows **all 60 metrics** organized into 7 categories:

1. **Robustness to Agent Errors and Outliers** (8 metrics)
2. **Consensus and Decision Quality** (8 metrics)
3. **Explainability and Transparency** (9 metrics)
4. **Error Types and Failure Modes** (8 metrics)
5. **Resource Utilization** (13 metrics)
6. **Quality Attribute Traceability** (7 metrics)
7. **User/Auditor Experience** (7 metrics)

Plus:
- System information (timestamp, problem, final answer, correctness)
- Summary statistics (accuracy, token efficiency, speed comparisons)

### Report Format

The report uses side-by-side tables showing each metric value for all systems, making it easy to compare:

```
Metric                              bMAS (LbMAS)          Static MAS           CoT
-----------------------------------------------------------------------------------
Agent Errors                        2                     1                     0
Error Corrections                   1                     0                     0
Resilience Episodes                 1                     0                     0
...
```

### Finding Metrics Files

The script automatically finds metrics files in the `metrics_outputs/` directory with the pattern:
- `{system_name}_metrics_{timestamp}.json`

Supported system names:
- `bMAS` → bMAS (LbMAS - Paper Prompts)
- `orig_impl_bMAS` → orig_impl_bMAS (Original Prompts)
- `Static_MAS` → Static MAS
- `CoT` → Chain-of-Thought (CoT)

### Customization

You can modify the script to:
- Change the metrics directory: Edit `metrics_dir` parameter in `find_recent_metrics_files()`
- Include more files per system: Change `max_files_per_system` parameter
- Change output filename: Edit `output_file` parameter in `generate_comparison_report()`

### Example Output

After running experiments, you'll get a report like:

```
================================================================================
COMPREHENSIVE METRICS COMPARISON REPORT - ALL 4 MAS SYSTEMS
================================================================================
Generated: 2024-01-15 14:30:00

================================================================================
SYSTEM INFORMATION
================================================================================

bMAS (LbMAS - Paper Prompts):
  File: metrics_outputs/bMAS_metrics_20240115_143000.json
  Timestamp: 2024-01-15T14:30:00
  Problem: In the land of Ink, the money system is unique...
  Final Answer: 6 Trinkets
  Correct: True

...

================================================================================
1. ROBUSTNESS TO AGENT ERRORS AND OUTLIERS
================================================================================
Metric                              bMAS (LbMAS)          Static MAS           CoT
-----------------------------------------------------------------------------------
Agent Errors                        2                     1                     0
Error Corrections                   1                     0                     0
...
```

### Requirements

- Python 3.6+
- Standard library only (no external dependencies)
- Metrics files generated by the metrics tracking system

### Notes

- The script uses the **most recent** metrics file from each system
- If a system has no metrics files, it will be skipped
- The report is saved as a text file for easy viewing and sharing
- All 60 metrics are included in the comparison

### Troubleshooting

**"No metrics files found"**
- Make sure you've run experiments with metrics tracking enabled
- Check that the `metrics_outputs/` directory exists
- Verify that metrics files follow the naming pattern: `{system}_metrics_{timestamp}.json`

**"Error loading metrics file"**
- Check that the JSON files are valid
- Ensure you have read permissions for the metrics files

### Integration with Experiment Scripts

You can integrate this into your experiment workflow:

```python
# After running experiments
from generate_metrics_comparison_report import generate_comparison_report, find_recent_metrics_files

system_files = find_recent_metrics_files()
report_path = generate_comparison_report(system_files, "my_comparison_report.txt")
print(f"Report saved to: {report_path}")
```

